{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: customtransformers.py\n",
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-overflow: scroll\n",
    "#| code-summary: \"Mostrar/esconder código\"\n",
    "#| code-fold: show\n",
    "# Este código é apenas uma reprodução do script original.\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DropConstantColumns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in sklearn.pipeline.Pipeline object.\n",
    "    It drops constant columns from a pandas dataframe object.\n",
    "    Important: the constant columns are found in the fit function and dropped in the transform function.\n",
    "    \"\"\"\n",
    "    def __init__(self, print_cols: bool = False, thresh: float = None, search: Union[float, int] = None, ignore_prefix: list[str] = [], also: list[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        print_cols: default = False. Determine whether the fit function should print the constant columns' names.\n",
    "        thresh: default = None. If any value occurs more than this fraction of the total number of rows, the column is considered constant.\n",
    "        Initiates the class.\n",
    "        \"\"\"\n",
    "        self.print_cols = print_cols\n",
    "        self.also = also\n",
    "        self.thresh = thresh\n",
    "        self.ignore_prefix = ignore_prefix\n",
    "        self.search = search\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame , y: None = None) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose constant columns should be removed.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the names of the columns to be removed in the transform function.\n",
    "        \"\"\"\n",
    "        if self.thresh is None:\n",
    "            self.constant_cols = [\n",
    "                col\n",
    "                for col in X.columns\n",
    "                if (\n",
    "                    ((X[col].nunique() == 1) | (col in self.also))\n",
    "                    & ~any([col.startswith(prefix) for prefix in self.ignore_prefix])\n",
    "                )\n",
    "            ]\n",
    "        elif self.search is None:\n",
    "            self.constant_cols = [\n",
    "                col\n",
    "                for col in X.columns\n",
    "                if (\n",
    "                    (X[col].value_counts(normalize=True).max() > self.thresh)\n",
    "                    & ~any([col.startswith(prefix) for prefix in self.ignore_prefix])\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            self.constant_cols = [\n",
    "                col\n",
    "                for col in X.columns\n",
    "                if (\n",
    "                    ((X[col]==self.search).sum()/X.shape[0] > self.thresh)\n",
    "                    & (~any([col.startswith(prefix) for prefix in self.ignore_prefix]))\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        if self.print_cols:\n",
    "            print(f\"{len(self.constant_cols)} constant columns were found.\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        X: dataset whose constant columns should be removed.\n",
    "        Returns dataset without the constant columns found in the fit function.\n",
    "        \"\"\"\n",
    "        return X.copy().drop(self.constant_cols, axis=1)\n",
    "\n",
    "class DropDuplicateColumns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in sklearn.pipeline.Pipeline object.\n",
    "    It drops duplicate columns from a pandas dataframe object.\n",
    "    Important: the duplicate columns are found in the fit function and dropped in the transform function.\n",
    "    \"\"\"\n",
    "    def __init__(self, print_cols: bool = False, ignore: list[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        print_cols: default = False. Determine whether the fit function should print the duplicate columns' names.\n",
    "        ignore: list of columns to ignore.\n",
    "        Initiates the class.\n",
    "        \"\"\"\n",
    "        self.print_cols = print_cols\n",
    "        self.ignore = ignore\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: None = None) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose duplicate columns should be removed.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the names of the columns to be removed in the transform function.\n",
    "        \"\"\"\n",
    "        regular_columns = []\n",
    "        duplicate_columns = []\n",
    "        sorted_cols = sorted(X.columns)\n",
    "        for col0 in sorted_cols:\n",
    "            if col0 not in duplicate_columns:\n",
    "                regular_columns.append(col0)\n",
    "            for col1 in sorted_cols:\n",
    "                if (col0 != col1):\n",
    "                    if X[col0].equals(X[col1]):\n",
    "                        if col1 not in regular_columns:\n",
    "                            duplicate_columns.append(col1)\n",
    "        self.duplicate_cols = duplicate_columns\n",
    "        if self.print_cols:\n",
    "            print(f\"{len(duplicate_columns)} duplicate columns were found.\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        X: dataset whose duplicate columns should be removed.\n",
    "        Returns dataset without the duplicate columns found in the fit function.\n",
    "        \"\"\" \n",
    "        X_ = X.copy()\n",
    "        return X_.drop(self.duplicate_cols, axis=1)\n",
    "\n",
    "class AddNonZeroCount(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in sklearn.pipeline.Pipeline object.\n",
    "    \"\"\"\n",
    "    def __init__(self, prefix: str = \"\", ignore: list[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        prefix: prefix of the columns to be summed.\n",
    "        ignore: list of columns to ignore.\n",
    "        fake_value: value to be replaced with None.\n",
    "        Initiates de class.\n",
    "        \"\"\"\n",
    "        self.prefix = prefix\n",
    "        self.ignore = ignore\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: None = None) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose \"prefix\" variables different than 0 should be counted.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the names of the columns whose not 0 values should be counted in the transform function.\n",
    "        \"\"\"\n",
    "        self.prefix_cols = [\n",
    "            col\n",
    "            for col in X.columns\n",
    "            if (\n",
    "                    (col.startswith(self.prefix))\n",
    "                    & (col not in self.ignore)\n",
    "            )\n",
    "        ]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        X: dataset whose \"prefix\" variables' not 0 values should be counted.\n",
    "        Returns dataset with new column with the count of the \"prefix\" variables' not 0 values.\n",
    "        \"\"\"  \n",
    "        X_ = X.copy()\n",
    "        X_[f\"non_zero_count_{self.prefix}\"] = X_[self.prefix_cols] \\\n",
    "            .applymap(lambda x: 0 if ((x == 0) | (x == None)) else 1) \\\n",
    "            .sum(axis=1)\n",
    "        return X_\n",
    "\n",
    "class CustomSum(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in sklearn.pipeline.Pipeline object.\n",
    "    It sums columns from a pandas dataframe object based on the columns prefix.\n",
    "    \"\"\"\n",
    "    def __init__(self, prefix: str = \"\", ignore: list[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        prefix: prefix of the columns to be summed.\n",
    "        ignore: list of columns to ignore.\n",
    "        fake_value: value to be replaced with None.\n",
    "        Initiates de class.\n",
    "        \"\"\"\n",
    "        self.prefix = prefix\n",
    "        self.ignore = ignore\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: None = None) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose columns with \"prefix\" should be summed.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the names of the columns to be summed in the transform function.\n",
    "        \"\"\"\n",
    "        self.prefix_cols = [\n",
    "            col\n",
    "            for col in X.columns\n",
    "            if (\n",
    "                    (col.startswith(self.prefix))\n",
    "                    & (col not in self.ignore)\n",
    "            )\n",
    "        ]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        X: dataset whose \"prefix\" variables should be summed.\n",
    "        Returns dataset with new column with the sum of the \"prefix\" variables.\n",
    "        \"\"\"  \n",
    "        X_ = X.copy()\n",
    "        X_[f\"sum_of_{self.prefix}\"] = X_[self.prefix_cols] \\\n",
    "            .sum(axis=1)\n",
    "        return X_\n",
    "\n",
    "class CustomImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in a sklearn.pipeline.Pipeline object.\n",
    "    It imputes values in a pandas dataframe object based on the columns prefix.\n",
    "    \"\"\"\n",
    "    def __init__(self, prefix: str, to_replace: Union[int, float, str],\n",
    "                 replace_with: Union[int, float, str] = np.nan, ignore: list[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        prefix: prefix of the columns to be imputed.\n",
    "        to_replace: value to be replaced.\n",
    "        replace_with: value to replace \"to_replace\" with.\n",
    "        ignore: list of columns to ignore.\n",
    "        Initiates de class.\n",
    "        \"\"\"\n",
    "        self.prefix = prefix\n",
    "        self.to_replace = to_replace\n",
    "        self.replace_with = replace_with\n",
    "        self.ignore = ignore\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, pd.Series], y: None = None) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose columns with \"prefix\" should be imputed.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the names of the columns to be imputed in the transform function.\n",
    "        \"\"\"\n",
    "        self.prefix_cols = [\n",
    "            col\n",
    "            for col in X.columns\n",
    "            if (\n",
    "                    (col.startswith(self.prefix))\n",
    "                    & (col not in self.ignore)\n",
    "            )\n",
    "        ]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: Union[pd.DataFrame, pd.Series]) -> Union[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        X: dataset whose columns with \"prefix\" should be imputed.\n",
    "        Returns dataset with the imputed columns.\n",
    "        \"\"\"\n",
    "        X_ = X.copy()\n",
    "        X_[self.prefix_cols] = X_[self.prefix_cols] \\\n",
    "            .replace(self.to_replace, self.replace_with)\n",
    "        return X_\n",
    " \n",
    "class AddNoneCount(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in sklearn.pipeline.Pipeline object.\n",
    "    It counts the number of None values in a pandas dataframe object based on the columns prefix.\n",
    "    \"\"\"\n",
    "    def __init__(self, prefix: str = \"\", ignore: list[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        prefix: subset of variables for none count starting with this string.\n",
    "        fake_value: values inserted to replace None.\n",
    "        ignore: list of columns with prefix to ignore.\n",
    "        drop_constant: whether to drop columns that would become constant without missing features or not.\n",
    "        \"\"\"\n",
    "        self.prefix = prefix\n",
    "        self.ignore = ignore\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: None = None) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose \"prefix\" variables' null values should be counted.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the names of the columns whose null values should be counted in the transform function.\n",
    "        \"\"\"\n",
    "        self.prefix_cols = [\n",
    "            col\n",
    "            for col in X.columns\n",
    "            if (\n",
    "                    (col.startswith(self.prefix))\n",
    "                    & (col not in self.ignore)\n",
    "            )\n",
    "        ]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        X: dataset to apply transformation on.\n",
    "        Returns dataset with new column with the count of the \"prefix\" variables' null values.\n",
    "        \"\"\"  \n",
    "        X_ = X.copy()\n",
    "        X_[f\"none_count_{self.prefix}\"] = X_[self.prefix_cols] \\\n",
    "            .isnull() \\\n",
    "            .sum(axis=1)\n",
    "        return X_\n",
    "    \n",
    "class CustomEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in sklearn.pipeline.Pipeline object.\n",
    "    It encodes categorical variables in a pandas dataframe based on the categories mean of the target variable.\n",
    "    Unknown values must be defined by the user.\n",
    "    \"\"\"\n",
    "    def __init__(self, colname: str) -> None:\n",
    "        \"\"\"\n",
    "        labels: dictionary with the labels to be replaced.\n",
    "        colname: name of the column to be encoded.\n",
    "        Initiates de class.\n",
    "        \"\"\"\n",
    "        self.colname = colname\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Union[pd.DataFrame, pd.Series]) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose column should be encoded.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the dictionary to be used in the transform function.\n",
    "        \"\"\"\n",
    "        X_ = X.copy().assign(TARGET=y)\n",
    "\n",
    "        grouped_X_ = X_ \\\n",
    "            .groupby(self.colname) \\\n",
    "            .agg({\"TARGET\": \"mean\"}) \\\n",
    "            .sort_values(\"TARGET\", ascending=True)\n",
    "        \n",
    "        groups = grouped_X_.index\n",
    "\n",
    "        self.labels ={\n",
    "            groups[i]: i\n",
    "            for i in range(len(groups))\n",
    "        }\n",
    "\n",
    "        self.most_frequent = X_[self.colname].mode()[0]\n",
    "        return self\n",
    "    \n",
    "    def _apply_map(self, x: Union[int, str]) -> int:\n",
    "        \"\"\"\n",
    "        x: value to be replaced.\n",
    "        Returns the value to replace \"x\" with.\n",
    "        \"\"\"\n",
    "        if x in self.labels.keys():\n",
    "            return self.labels[x]\n",
    "        else:\n",
    "            return self.labels[self.most_frequent]\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        X: dataset whose column should be encoded.\n",
    "        Returns dataset with the encoded column.\n",
    "        \"\"\"\n",
    "        X_ = X.copy()\n",
    "        X_[self.colname] = X_[self.colname] \\\n",
    "            .apply(self._apply_map)\n",
    "        return X_\n",
    "    \n",
    "class CustomLog(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns: list[str] = []) -> None:\n",
    "        self.columns = columns\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        X_[self.columns] = np.log1p(\n",
    "            X_[self.columns] - X_[self.columns].min()\n",
    "        )\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: customviz.py\n",
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-overflow: scroll\n",
    "#| code-summary: \"Mostrar/esconder código\"\n",
    "#| code-fold: true\n",
    "# Este código é apenas uma reprodução do script original.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def binaryhistplot(x: pd.Series, hue: pd.Series, nbins=25) -> None:\n",
    "    sns.histplot(\n",
    "        x=x,\n",
    "        hue=hue,\n",
    "        element=\"step\",\n",
    "        stat=\"density\",\n",
    "        common_norm=False,\n",
    "        common_bins=True,\n",
    "        bins=nbins\n",
    "    )\n",
    "    plt.title(f\"Distribution of {x.name} var by TARGET\")\n",
    "    plt.show()\n",
    "\n",
    "def expl_var(evr) -> None:\n",
    "    cumulative_variance = np.cumsum(evr)\n",
    "    index_80_percent = np.where(cumulative_variance >= 0.8)[0][0]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    ax1.bar(range(1, len(evr)+1), evr, alpha=0.8, align='center')\n",
    "    ax1.set_ylabel('Explained Variance Ratio', color='b')\n",
    "    ax1.set_xlabel('Principal Component')\n",
    "    for label in ax1.get_yticklabels():\n",
    "        label.set_color(\"b\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.step(range(1, len(evr)+1), cumulative_variance, where='mid', label='Cumulative Explained Variance', color='g')\n",
    "    ax2.axvline(x=index_80_percent + 1, color='r', linestyle='--', label='80% of Explained Variance')\n",
    "    ax2.set_ylabel('Cumulative Explained Variance', color='g')\n",
    "    for label in ax2.get_yticklabels():\n",
    "        label.set_color(\"g\")\n",
    "\n",
    "    ax1.set_ylim([0, max(evr)*1.1])\n",
    "    ax2.set_ylim([0, 1])\n",
    "\n",
    "    ax1.set_xlim([0, 30])\n",
    "    ax2.set_xlim([0, 30])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"80% of variance is explained by {index_80_percent + 1} components\")\n",
    "    \n",
    "def plot_components(components, feature_names) -> None:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, component in enumerate(components):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        plt.bar(feature_names, component)\n",
    "        plt.title(f\"Component {i+1}\")\n",
    "        plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: edautils.py\n",
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-overflow: scroll\n",
    "#| code-summary: \"Mostrar/esconder código\"\n",
    "#| code-fold: true\n",
    "# Este código é apenas uma reprodução do script original.\n",
    "import pandas as pd\n",
    "\n",
    "def neg_pos_zero(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a dataframe with the percentage of negative, positive and zero values in the given columns.\n",
    "    :param df: Dataframe to analyze.\n",
    "    :param cols: Columns to analyze.\n",
    "    :return: Dataframe with the percentage of negative, positive and zero values in the given columns.\n",
    "    \"\"\"\n",
    "    return df[cols] \\\n",
    "        .applymap(lambda x: 100 / len(df) if x < 0 else 0) \\\n",
    "        .sum() \\\n",
    "        .reset_index() \\\n",
    "        .rename(\n",
    "            mapper={\n",
    "                \"index\": \"Column\",\n",
    "                0: \"Negative values (%)\"\n",
    "            },\n",
    "            axis=1\n",
    "        ) \\\n",
    "        .merge(\n",
    "            df[cols] \\\n",
    "                .applymap(lambda x: 100 / len(df) if x > 0 else 0) \\\n",
    "                .sum() \\\n",
    "                .reset_index() \\\n",
    "                .rename(\n",
    "                    mapper={\n",
    "                        \"index\": \"Column\",\n",
    "                        0: \"Positive values (%)\"\n",
    "                    },\n",
    "                    axis=1\n",
    "                ),\n",
    "            on=\"Column\",\n",
    "            how=\"outer\"\n",
    "        ) \\\n",
    "        .merge(\n",
    "            df[cols] \\\n",
    "                .applymap(lambda x: 100 / len(df) if x == 0 else 0) \\\n",
    "                .sum() \\\n",
    "                .reset_index() \\\n",
    "                .rename(\n",
    "                    mapper={\n",
    "                        \"index\": \"Column\",\n",
    "                        0: \"Zero values (%)\"\n",
    "                    },\n",
    "                    axis=1\n",
    "                ),\n",
    "            on=\"Column\",\n",
    "            how=\"outer\"\n",
    "        ) \\\n",
    "        .sort_values(\"Negative values (%)\")\n",
    "\n",
    "def npz_median(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    npz = neg_pos_zero(df, cols) \\\n",
    "        .set_index(\"Column\") \\\n",
    "        .median() \\\n",
    "        .reset_index() \\\n",
    "        .set_index(\"index\") \\\n",
    "        .rename(mapper={0: \"Mediana\"}, axis=1)\n",
    "    npz.index.name = \"Medida\"\n",
    "    return npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: prep.py\n",
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-overflow: scroll\n",
    "#| code-summary: \"Mostrar/esconder código\"\n",
    "#| code-fold: true\n",
    "# Este código é apenas uma reprodução do script original.\n",
    "# --- Transformers --- #\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import \\\n",
    "    StandardScaler, \\\n",
    "    OneHotEncoder\n",
    "from resources.customtransformers import \\\n",
    "    DropConstantColumns, \\\n",
    "    DropDuplicateColumns, \\\n",
    "    AddNonZeroCount, \\\n",
    "    CustomSum, \\\n",
    "    CustomImputer, \\\n",
    "    AddNoneCount, \\\n",
    "    CustomEncoder, \\\n",
    "    CustomLog\n",
    "\n",
    "# --- Pipeline Building --- #\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def build_prep() -> Pipeline:\n",
    "    \"\"\"\n",
    "    Builds base pipeline.\n",
    "    \"\"\"\n",
    "    prep = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"DropConstantColumns\",\n",
    "                DropConstantColumns(also=[\"ID\"])\n",
    "            ),\n",
    "            (\n",
    "                \"DropDuplicateColumns\",\n",
    "                DropDuplicateColumns()\n",
    "            ),\n",
    "            (\n",
    "                \"NoneZeroCountSaldo\",\n",
    "                AddNonZeroCount(prefix=\"saldo\")\n",
    "            ),\n",
    "            (\n",
    "                \"SumSaldo\",\n",
    "                CustomSum(prefix=\"saldo\")\n",
    "            ),\n",
    "            (\n",
    "                \"NoneZeroCountImp\",\n",
    "                AddNonZeroCount(prefix=\"imp\")\n",
    "            ),\n",
    "            (\n",
    "                \"SumImp\",\n",
    "                CustomSum(prefix=\"imp\")\n",
    "            ),\n",
    "            (\n",
    "                \"ImputeNanDelta\",\n",
    "                CustomImputer(prefix=\"delta\", to_replace=9999999999)\n",
    "            ),\n",
    "            (\n",
    "                \"NoneCountDelta\",\n",
    "                AddNoneCount(prefix=\"delta\")\n",
    "            ),\n",
    "            (\n",
    "                \"NonZeroCountDelta\",\n",
    "                AddNonZeroCount(prefix=\"delta\")\n",
    "            ),\n",
    "            (\n",
    "                \"SumDelta\",\n",
    "                CustomSum(prefix=\"delta\")\n",
    "            ),\n",
    "            (\n",
    "                \"NonZeroContInd\",\n",
    "                AddNonZeroCount(prefix=\"ind\")\n",
    "            ),\n",
    "            (\n",
    "                \"NonZeroCountNum\",\n",
    "                AddNonZeroCount(prefix=\"num\")\n",
    "            ),\n",
    "            (\n",
    "                \"SumNum\",\n",
    "                CustomSum(prefix=\"num\")\n",
    "            ),\n",
    "            (\n",
    "                \"ImputeNanVar3\",\n",
    "                CustomImputer(prefix=\"var3\", to_replace=-999999)\n",
    "            ),\n",
    "            (\n",
    "                \"CustomEncoderVar36\",\n",
    "                CustomEncoder(colname=\"var36\")\n",
    "            ),\n",
    "            (\n",
    "                \"CustomEncoderVar21\",\n",
    "                CustomEncoder(colname=\"var21\")\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return prep\n",
    "\n",
    "def build_prep_2() -> Pipeline:\n",
    "    prep = Pipeline(\n",
    "        steps=[\n",
    "            (\"prep\", build_prep()),\n",
    "            (\"NoneCountVar3\", AddNoneCount(prefix=\"var3\")),\n",
    "            (\"drop_almost\", DropConstantColumns(thresh=.99, ignore_prefix=[\"ind\"])),\n",
    "            (\"nan\", SimpleImputer(strategy=\"median\"))\n",
    "        ]\n",
    "    )\n",
    "    return prep\n",
    "\n",
    "def build_prep_3(n_comp=None) -> Pipeline:\n",
    "    log_cols = [\n",
    "        'var3',\n",
    "        'saldo_var30',\n",
    "        'saldo_var42',\n",
    "        'saldo_medio_var5_hace2',\n",
    "        'saldo_medio_var5_hace3',\n",
    "        'saldo_medio_var5_ult1',\n",
    "        'saldo_medio_var5_ult3',\n",
    "        'num_var42_0',\n",
    "        'sum_of_saldo',\n",
    "        'var38',\n",
    "        'sum_of_num',\n",
    "        'non_zero_count_num',\n",
    "        'non_zero_count_ind'\n",
    "    ]\n",
    "    \n",
    "    cat_cols = [\"var36\"]\n",
    "\n",
    "    cat_tf = Pipeline(\n",
    "        steps=[\n",
    "            (\"ohe\", OneHotEncoder(min_frequency=100, sparse_output=False)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prep = Pipeline(\n",
    "        steps=[\n",
    "            (\"prep\", build_prep()[:-2]),\n",
    "            (\"drop_almost\", DropConstantColumns(thresh=.4, search=0)),\n",
    "            (\"log\", CustomLog(columns = log_cols)),\n",
    "            (\"cat\",ColumnTransformer([(\"ohe\", cat_tf, cat_cols)], remainder='passthrough')),\n",
    "            (\"ss\", StandardScaler()),\n",
    "            (\"knn\", KNNImputer(n_neighbors=5)),\n",
    "            (\"pca\", PCA(n_components=n_comp))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: split.py\n",
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-overflow: scroll\n",
    "#| code-summary: \"Mostrar/esconder código\"\n",
    "#| code-fold: true\n",
    "# Este código é apenas uma reprodução do script original.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"data/raw.csv\")\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.25, random_state=42, stratify=df[\"TARGET\"])\n",
    "\n",
    "train.to_csv(\"data/train.csv\", index=False)\n",
    "test.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "print(\"Train and test sets saved to data/ folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: train_evaluate.py\n",
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-overflow: scroll\n",
    "#| code-summary: \"Mostrar/esconder código\"\n",
    "#| code-fold: true\n",
    "# Este código é apenas uma reprodução do script original.\n",
    "# --- Function Building --- #\n",
    "from typing import Union\n",
    "\n",
    "# --- Threshold Optimization --- #\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# --- Data Manipulation --- #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- sklearn utils --- #\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import \\\n",
    "    train_test_split, \\\n",
    "    GridSearchCV, \\\n",
    "    StratifiedKFold\n",
    "\n",
    "# --- sklearn metrics --- #\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import \\\n",
    "    roc_auc_score, \\\n",
    "    confusion_matrix, \\\n",
    "    accuracy_score, \\\n",
    "    precision_score, \\\n",
    "    recall_score, \\\n",
    "    f1_score\n",
    "\n",
    "# --- Object serialization --- #\n",
    "import pickle\n",
    "\n",
    "class TrainEvaluate:\n",
    "    \"\"\"\n",
    "    This class can be used to train, validate and test sklearn Pipeline objects.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: Pipeline, param_grid: dict, target: str,\n",
    "                 njobs: int = 8, verbose: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        model: sklearn Pipeline with the model.\n",
    "        param_grid: Dictionary of parameters to search over.\n",
    "        target: Name of the column to predict.\n",
    "        save_model: Wheter to save the model or not.\n",
    "        save_name: Name of the file to save the model.\n",
    "        njobs: Number of jobs to run in parallel.\n",
    "        verbose: Wheter to print the progress or not.\n",
    "        Initialize the class with the model, param_grid, and target variable.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.param_grid = param_grid\n",
    "        self.target = target\n",
    "        self.njobs = njobs\n",
    "        self.verbose = verbose\n",
    "        pass\n",
    "\n",
    "    def _validation_split(self, df: pd.DataFrame) -> tuple:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the data.\n",
    "        Split the data into train and validation sets.\n",
    "        \"\"\"\n",
    "        y = df[self.target]\n",
    "        X = df.drop(self.target, axis=1)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=0.25,\n",
    "            random_state=42\n",
    "        )\n",
    "        return (X_train, X_val, y_train, y_val)\n",
    "    \n",
    "    def _grid_search(self, X_train: pd.DataFrame, y_train: Union[pd.DataFrame, pd.Series]) -> GridSearchCV:\n",
    "        \"\"\"\n",
    "        X_train: Pandas DataFrame with the training data.\n",
    "        y_train: Pandas Series with the training target.\n",
    "        \"\"\"\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=self.model,\n",
    "            param_grid=self.param_grid,\n",
    "            scoring=\"roc_auc\",\n",
    "            n_jobs=self.njobs,\n",
    "            cv=skf,\n",
    "            verbose=3\n",
    "        )\n",
    "        grid_search = grid_search.fit(X_train, y_train)\n",
    "        return grid_search\n",
    "    \n",
    "    def _profit(self, y_true: Union[np.ndarray, pd.DataFrame, pd.Series],\n",
    "                y_pred: Union[np.ndarray, pd.DataFrame, pd.Series]) -> float:\n",
    "        \"\"\"\n",
    "        y_true: Pandas Series with the true target.\n",
    "        y_pred: Pandas Series with the predicted target.\n",
    "        Calculate the profit metric of the model.\n",
    "        \"\"\"\n",
    "        tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        n = len(y_true)\n",
    "        profit = (90 * tp - 10 * fp)\n",
    "        return profit\n",
    "    \n",
    "    def _threshold_tuning(self, X_val: pd.DataFrame, y_val: Union[pd.DataFrame, pd.Series]) -> float:\n",
    "        \"\"\"\n",
    "        X_val: Pandas DataFrame with the validation data.\n",
    "        y_val: Pandas Series with the validation target.\n",
    "        Find the threshold that maximizes the profit metric.\n",
    "        \"\"\"\n",
    "        y_proba = self.best_model_.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        def profit_treshold(x: float) -> float:\n",
    "            \"\"\"\n",
    "            x: Threshold to test.\n",
    "            Returns negative of the profit metric.\n",
    "            \"\"\"\n",
    "            y_pred = (y_proba >= x).astype(int)\n",
    "            scalar = -self._profit(y_val, y_pred)\n",
    "            return scalar\n",
    "        \n",
    "        threshold = minimize_scalar(\n",
    "            profit_treshold,\n",
    "            bounds=(0, 1),\n",
    "            method=\"bounded\"\n",
    "        )\n",
    "        self.threshold = threshold.x\n",
    "        return threshold.x\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the data.\n",
    "        path: Path to a fitted model.\n",
    "        Splits data between train and validation, performs GridSearchCV,\n",
    "        adjusts the threshold based on profit metric on the validation set,\n",
    "        and fits the model on the original data.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Splitting data into train and validation sets...\")\n",
    "        X_train, X_val, y_train, y_val = self._validation_split(df)\n",
    "        if self.verbose:\n",
    "            print(\"Done!\")\n",
    "            print(\"Performing GridSearchCV...\")\n",
    "        self.best_model_ = self._grid_search(X_train, y_train).best_estimator_\n",
    "        if self.verbose:\n",
    "            print(\"Done!\")\n",
    "            print(\"Adjusting threshold based on validation set...\")\n",
    "        self.threshold = self._threshold_tuning(X_val, y_val)\n",
    "        if self.verbose:\n",
    "            print(\"Done!\")\n",
    "            print(\"Fitting model on the whole dataset...\")\n",
    "        self.best_model_ = self.best_model_.fit(df.drop(self.target, axis=1), df[self.target])\n",
    "        if self.verbose:\n",
    "            print(\"Done!\")\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the data.\n",
    "        Predicts the target variable using the best model.\n",
    "        \"\"\"\n",
    "        return self.best_model_.predict_proba(df)[:, 1]\n",
    "    \n",
    "    def predict(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the data.\n",
    "        Predicts the target variable using the best model and the threshold.\n",
    "        \"\"\"\n",
    "        y_proba = self.predict_proba(df)\n",
    "        y_pred = (y_proba >= self.threshold).astype(int)\n",
    "        return y_pred\n",
    "    \n",
    "    def evaluate(self, df: pd.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the test data.\n",
    "        Evaluates the model on the data.\n",
    "        \"\"\"\n",
    "        X_test = df.drop(self.target, axis=1)\n",
    "        y_true = df[self.target]\n",
    "        y_proba = self.predict_proba(X_test)\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "        self.business_metrics = {\n",
    "            \"Profit (Total)\": tp * 90 - fp * 10,\n",
    "            \"Profit (per Customer)\": (tp * 90 - fp * 10) / len(y_true),\n",
    "            \"True Positive Profit (Total)\": tp * 90,\n",
    "            \"True Positive Profit (per Customer)\": tp * 90 / len(y_true),\n",
    "            \"False Positive Loss (Total)\": fp * 10,\n",
    "            \"False Positive Loss (per Customer)\": fp * 10 / len(y_true),\n",
    "            \"False Negative Potential Profit Loss (Total)\": fn * 90,\n",
    "            \"False Negative Potential Profit Loss (per Customer)\": fn * 90 / len(y_true),\n",
    "            \"True Negative Loss Prevention (Total)\": tn * 10,\n",
    "            \"True Negative Loss Prevention (per Customer)\": tn * 10 / len(y_true)\n",
    "        }\n",
    "\n",
    "        self.classification_metrics = {\n",
    "            \"Classification Threshold\": self.threshold,\n",
    "            \"ROC AUC\": roc_auc_score(y_true, y_proba),\n",
    "            \"Precision\": precision_score(y_true, y_pred),\n",
    "            \"Recall\": recall_score(y_true, y_pred),\n",
    "            \"F1\": f1_score(y_true, y_pred),\n",
    "            \"Accuracy\": accuracy_score(y_true, y_pred)\n",
    "        }\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict_profit(self, model, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        X: Pandas DataFrame with the data.\n",
    "        y: Pandas Series with the target.\n",
    "        Predicts the profit metric using the best model and custom threshold.\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X)\n",
    "        return self._profit(y, y_pred)\n",
    "\n",
    "    def get_feature_importances(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the data.\n",
    "        Implements permutation feature importances on the data using the best model and custom threshold.\n",
    "        \"\"\"\n",
    "    \n",
    "        X = df.drop(self.target, axis=1)\n",
    "        X = self.best_model_.steps[0][1].transform(X)\n",
    "        y = df[self.target]\n",
    "        result = permutation_importance(\n",
    "            self.best_model_.steps[1][1],\n",
    "            X,\n",
    "            y,\n",
    "            scoring=self._predict_profit,\n",
    "            n_repeats=30,\n",
    "            random_state=42,\n",
    "            n_jobs=self.njobs\n",
    "        )\n",
    "        feature_importances = pd.DataFrame({\n",
    "            \"Feature\": X.columns,\n",
    "            \"Importance\": result.importances_mean\n",
    "        })\n",
    "        self.feature_importances = feature_importances.sort_values(\"Importance\", ascending=False)\n",
    "        return self.feature_importances\n",
    "    \n",
    "    def rank_customers(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the data.\n",
    "        Ranks the customers by their probability of insatisfaction.\n",
    "        \"\"\"\n",
    "        df_ = df.copy()\n",
    "        X = df_.drop(self.target, axis=1)\n",
    "        y = df_[self.target]\n",
    "\n",
    "        def apply_rank(x: float) -> int:\n",
    "            \"\"\"\n",
    "            x: Probability of insatisfaction.\n",
    "            Applies the rank (1 to 5) to the probability of insatisfaction.\n",
    "            \"\"\"\n",
    "            thresholds = [c * self.threshold / 4 for c in range(5)][::-1]\n",
    "            for rank, threshold in enumerate(thresholds):\n",
    "                if x >= threshold:\n",
    "                    return rank + 1\n",
    "            return 5\n",
    "\n",
    "        df_[\"rank\"] = self.predict_proba(X)\n",
    "        return df_[\"rank\"].apply(apply_rank)\n",
    "    \n",
    "def build_model(path: str = None, train_df: pd.DataFrame = None, model: Pipeline = None,\n",
    "                param_grid: dict = None, target: str = None,\n",
    "                njobs: int = 8, verbose: bool = True) -> TrainEvaluate:\n",
    "    \"\"\"\n",
    "    path: Path to a fitted model.\n",
    "    train_df: Pandas DataFrame with the training data.\n",
    "    model: sklearn Pipeline with the model.\n",
    "    param_grid: Dictionary of parameters to search over.\n",
    "    target: Name of the column to predict.\n",
    "    njobs: Number of jobs to run in parallel.\n",
    "    verbose: Wheter to print the progress or not.\n",
    "    Builds a TrainEvaluate object.\n",
    "    \"\"\"\n",
    "\n",
    "    train_evaluate = TrainEvaluate(model, param_grid, target, njobs, verbose)\n",
    "    train_evaluate = train_evaluate.fit(train_df)\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(train_evaluate, f)\n",
    "    return train_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: 11-rf.py\n",
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-overflow: scroll\n",
    "#| code-summary: \"Mostrar/esconder código\"\n",
    "#| code-fold: true\n",
    "# Este código é apenas uma reprodução do script original.\n",
    "import pandas as pd\n",
    "from resources.prep import build_prep_2\n",
    "from resources.train_evaluate import build_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"Training RandomForestClassifier...\")\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "rf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", build_prep_2()),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            RandomForestClassifier(\n",
    "                random_state=42,\n",
    "                n_estimators=500,\n",
    "                class_weight=\"balanced\"\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf_grid = {\n",
    "    \"classifier__max_depth\": [4, 8, 16, 32],\n",
    "    \"classifier__max_features\": [8, 16, 32, 64],\n",
    "}\n",
    "\n",
    "rf_model = build_model(\n",
    "    path = \"models/rf.pkl\",\n",
    "    train_df = train,\n",
    "    model = rf,\n",
    "    param_grid = rf_grid,\n",
    "    target = \"TARGET\",\n",
    "    njobs = 8,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "print(\"RandomForestClassifier trained. Model saved to models/rf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: 12-hgb.py\n",
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-overflow: scroll\n",
    "#| code-summary: \"Mostrar/esconder código\"\n",
    "#| code-fold: true\n",
    "# Este código é apenas uma reprodução do script original.\n",
    "import pandas as pd\n",
    "from resources.prep import build_prep\n",
    "from resources.train_evaluate import build_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as HGB\n",
    "\n",
    "print(\"Training HistGradientBoostingClassifier...\")\n",
    "\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "hgb = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", build_prep()),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            HGB(\n",
    "                random_state=42,\n",
    "                class_weight=\"balanced\",\n",
    "                categorical_features=[\"var36\", \"var21\"]\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "hgb_grid = {\n",
    "    \"classifier__learning_rate\": [.01, .03, .1],\n",
    "    \"classifier__max_iter\": [100, 150, 200],\n",
    "    \"classifier__max_depth\": [3, 4, 5, 6],\n",
    "    \"classifier__l2_regularization\": [1, 3, 10]\n",
    "}\n",
    "\n",
    "hgbc_model = build_model(\n",
    "    path = \"models/hgb.pkl\",\n",
    "    train_df = df,\n",
    "    model = hgb,\n",
    "    param_grid = hgb_grid,\n",
    "    target = \"TARGET\",\n",
    "    njobs = 8,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "print(\"Done training HistGradientBoostingClassifier. Model saved to models/hgb.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmc-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
