{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: custom-transformers\n",
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-overflow: scroll\n",
    "#| code-summary: \"Mostrar/esconder código\"\n",
    "#| code-fold: true\n",
    "# Este código é apenas um exemplo. Os transformadores estão disponíveis no repositório deste projeto.\n",
    "class DropConstantColumns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in sklearn.pipeline.Pipeline object.\n",
    "    It drops constant columns from a pandas dataframe object.\n",
    "    Important: the constant columns are found in the fit function and dropped in the transform function.\n",
    "    \"\"\"\n",
    "    def __init__(self, print_cols: bool = False, also: list[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        print_cols: default = False. Determine whether the fit function should print the constant columns' names.\n",
    "        ignore: list of columns to ignore.\n",
    "        Initiates the class.\n",
    "        \"\"\"\n",
    "        self.print_cols = print_cols\n",
    "        self.also = also\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame , y: None = None) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose constant columns should be removed.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the names of the columns to be removed in the transform function.\n",
    "        \"\"\"\n",
    "        self.constant_cols = [\n",
    "            col\n",
    "            for col in X.columns\n",
    "            if (\n",
    "                (X[col].nunique() == 1)\n",
    "                | (col in self.also)\n",
    "            )\n",
    "        ]\n",
    "        if self.print_cols:\n",
    "            print(f\"{len(self.constant_cols)} constant columns were found.\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        X: dataset whose constant columns should be removed.\n",
    "        Returns dataset without the constant columns found in the fit function.\n",
    "        \"\"\"\n",
    "        return X.copy().drop(self.constant_cols, axis=1)\n",
    "\n",
    "class DropDuplicateColumns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in sklearn.pipeline.Pipeline object.\n",
    "    It drops duplicate columns from a pandas dataframe object.\n",
    "    Important: the duplicate columns are found in the fit function and dropped in the transform function.\n",
    "    \"\"\"\n",
    "    def __init__(self, print_cols: bool = False, ignore: list[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        print_cols: default = False. Determine whether the fit function should print the duplicate columns' names.\n",
    "        ignore: list of columns to ignore.\n",
    "        Initiates the class.\n",
    "        \"\"\"\n",
    "        self.print_cols = print_cols\n",
    "        self.ignore = ignore\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: None = None) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose duplicate columns should be removed.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the names of the columns to be removed in the transform function.\n",
    "        \"\"\"\n",
    "        regular_columns = []\n",
    "        duplicate_columns = []\n",
    "        sorted_cols = sorted(X.columns)\n",
    "        for col0 in sorted_cols:\n",
    "            if col0 not in duplicate_columns:\n",
    "                regular_columns.append(col0)\n",
    "            for col1 in sorted_cols:\n",
    "                if (col0 != col1):\n",
    "                    if X[col0].equals(X[col1]):\n",
    "                        if col1 not in regular_columns:\n",
    "                            duplicate_columns.append(col1)\n",
    "        self.duplicate_cols = duplicate_columns\n",
    "        if self.print_cols:\n",
    "            print(f\"{len(duplicate_columns)} duplicate columns were found.\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        X: dataset whose duplicate columns should be removed.\n",
    "        Returns dataset without the duplicate columns found in the fit function.\n",
    "        \"\"\" \n",
    "        X_ = X.copy()\n",
    "        return X_.drop(self.duplicate_cols, axis=1)\n",
    "\n",
    "class AddNonZeroCount(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in sklearn.pipeline.Pipeline object.\n",
    "    \"\"\"\n",
    "    def __init__(self, prefix: str = \"\", ignore: list[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        prefix: prefix of the columns to be summed.\n",
    "        ignore: list of columns to ignore.\n",
    "        fake_value: value to be replaced with None.\n",
    "        Initiates de class.\n",
    "        \"\"\"\n",
    "        self.prefix = prefix\n",
    "        self.ignore = ignore\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: None = None) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose \"prefix\" variables different than 0 should be counted.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the names of the columns whose not 0 values should be counted in the transform function.\n",
    "        \"\"\"\n",
    "        self.prefix_cols = [\n",
    "            col\n",
    "            for col in X.columns\n",
    "            if (\n",
    "                    (col.startswith(self.prefix))\n",
    "                    & (col not in self.ignore)\n",
    "            )\n",
    "        ]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        X: dataset whose \"prefix\" variables' not 0 values should be counted.\n",
    "        Returns dataset with new column with the count of the \"prefix\" variables' not 0 values.\n",
    "        \"\"\"  \n",
    "        X_ = X.copy()\n",
    "        X_[f\"non_zero_count_{self.prefix}\"] = X_[self.prefix_cols] \\\n",
    "            .applymap(lambda x: 0 if ((x == 0) | (x == None)) else 1) \\\n",
    "            .sum(axis=1)\n",
    "        return X_\n",
    "\n",
    "class CustomSum(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in sklearn.pipeline.Pipeline object.\n",
    "    It sums columns from a pandas dataframe object based on the columns prefix.\n",
    "    \"\"\"\n",
    "    def __init__(self, prefix: str = \"\", ignore: list[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        prefix: prefix of the columns to be summed.\n",
    "        ignore: list of columns to ignore.\n",
    "        fake_value: value to be replaced with None.\n",
    "        Initiates de class.\n",
    "        \"\"\"\n",
    "        self.prefix = prefix\n",
    "        self.ignore = ignore\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: None = None) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose columns with \"prefix\" should be summed.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the names of the columns to be summed in the transform function.\n",
    "        \"\"\"\n",
    "        self.prefix_cols = [\n",
    "            col\n",
    "            for col in X.columns\n",
    "            if (\n",
    "                    (col.startswith(self.prefix))\n",
    "                    & (col not in self.ignore)\n",
    "            )\n",
    "        ]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        X: dataset whose \"prefix\" variables should be summed.\n",
    "        Returns dataset with new column with the sum of the \"prefix\" variables.\n",
    "        \"\"\"  \n",
    "        X_ = X.copy()\n",
    "        X_[f\"sum_of_{self.prefix}\"] = X_[self.prefix_cols] \\\n",
    "            .sum(axis=1)\n",
    "        return X_\n",
    "\n",
    "class CustomImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in a sklearn.pipeline.Pipeline object.\n",
    "    It imputes values in a pandas dataframe object based on the columns prefix.\n",
    "    \"\"\"\n",
    "    def __init__(self, prefix: str, to_replace: Union[int, float, str],\n",
    "                 replace_with: Union[int, float, str] = np.nan, ignore: list[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        prefix: prefix of the columns to be imputed.\n",
    "        to_replace: value to be replaced.\n",
    "        replace_with: value to replace \"to_replace\" with.\n",
    "        ignore: list of columns to ignore.\n",
    "        Initiates de class.\n",
    "        \"\"\"\n",
    "        self.prefix = prefix\n",
    "        self.to_replace = to_replace\n",
    "        self.replace_with = replace_with\n",
    "        self.ignore = ignore\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, pd.Series], y: None = None) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose columns with \"prefix\" should be imputed.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the names of the columns to be imputed in the transform function.\n",
    "        \"\"\"\n",
    "        self.prefix_cols = [\n",
    "            col\n",
    "            for col in X.columns\n",
    "            if (\n",
    "                    (col.startswith(self.prefix))\n",
    "                    & (col not in self.ignore)\n",
    "            )\n",
    "        ]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: Union[pd.DataFrame, pd.Series]) -> Union[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        X: dataset whose columns with \"prefix\" should be imputed.\n",
    "        Returns dataset with the imputed columns.\n",
    "        \"\"\"\n",
    "        X_ = X.copy()\n",
    "        X_[self.prefix_cols] = X_[self.prefix_cols] \\\n",
    "            .replace(self.to_replace, self.replace_with)\n",
    "        return X_\n",
    " \n",
    "class AddNoneCount(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in sklearn.pipeline.Pipeline object.\n",
    "    It counts the number of None values in a pandas dataframe object based on the columns prefix.\n",
    "    \"\"\"\n",
    "    def __init__(self, prefix: str = \"\", ignore: list[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        prefix: subset of variables for none count starting with this string.\n",
    "        fake_value: values inserted to replace None.\n",
    "        ignore: list of columns with prefix to ignore.\n",
    "        drop_constant: whether to drop columns that would become constant without missing features or not.\n",
    "        \"\"\"\n",
    "        self.prefix = prefix\n",
    "        self.ignore = ignore\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: None = None) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose \"prefix\" variables' null values should be counted.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the names of the columns whose null values should be counted in the transform function.\n",
    "        \"\"\"\n",
    "        self.prefix_cols = [\n",
    "            col\n",
    "            for col in X.columns\n",
    "            if (\n",
    "                    (col.startswith(self.prefix))\n",
    "                    & (col not in self.ignore)\n",
    "            )\n",
    "        ]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        X: dataset to apply transformation on.\n",
    "        Returns dataset with new column with the count of the \"prefix\" variables' null values.\n",
    "        \"\"\"  \n",
    "        X_ = X.copy()\n",
    "        X_[f\"none_count_{self.prefix}\"] = X_[self.prefix_cols] \\\n",
    "            .isnull() \\\n",
    "            .sum(axis=1)\n",
    "        return X_\n",
    "    \n",
    "class CustomEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class is made to work as a step in sklearn.pipeline.Pipeline object.\n",
    "    It encodes categorical variables in a pandas dataframe based on the categories mean of the target variable.\n",
    "    Unknown values must be defined by the user.\n",
    "    \"\"\"\n",
    "    def __init__(self, colname: str) -> None:\n",
    "        \"\"\"\n",
    "        labels: dictionary with the labels to be replaced.\n",
    "        colname: name of the column to be encoded.\n",
    "        Initiates de class.\n",
    "        \"\"\"\n",
    "        self.colname = colname\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Union[pd.DataFrame, pd.Series]) -> None:\n",
    "        \"\"\"\n",
    "        X: dataset whose column should be encoded.\n",
    "        y: Shouldn't be used. Only exists to prevent raise Exception due to accidental input in a pipeline.\n",
    "        Creates class atributte with the dictionary to be used in the transform function.\n",
    "        \"\"\"\n",
    "        X_ = X.copy().assign(TARGET=y)\n",
    "\n",
    "        grouped_X_ = X_ \\\n",
    "            .groupby(self.colname) \\\n",
    "            .agg({\"TARGET\": \"mean\"}) \\\n",
    "            .sort_values(\"TARGET\", ascending=True)\n",
    "        \n",
    "        groups = grouped_X_.index\n",
    "\n",
    "        self.labels ={\n",
    "            groups[i]: i\n",
    "            for i in range(len(groups))\n",
    "        }\n",
    "\n",
    "        self.most_frequent = X_[self.colname].mode()[0]\n",
    "        return self\n",
    "    \n",
    "    def _apply_map(self, x: Union[int, str]) -> int:\n",
    "        \"\"\"\n",
    "        x: value to be replaced.\n",
    "        Returns the value to replace \"x\" with.\n",
    "        \"\"\"\n",
    "        if x in self.labels.keys():\n",
    "            return self.labels[x]\n",
    "        else:\n",
    "            return self.labels[self.most_frequent]\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        X: dataset whose column should be encoded.\n",
    "        Returns dataset with the encoded column.\n",
    "        \"\"\"\n",
    "        X_ = X.copy()\n",
    "        X_[self.colname] = X_[self.colname] \\\n",
    "            .apply(self._apply_map)\n",
    "        return X_\n",
    "    \n",
    "class PrefixScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, prefixes, scaler, zero_heavy = False, ignore=[]):\n",
    "        self.prefixes = prefixes\n",
    "        self.scaler = scaler\n",
    "        self.ignore = ignore\n",
    "        self.zero_heavy = zero_heavy\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.prefix_cols = [\n",
    "            col\n",
    "            for col in X.columns\n",
    "            if (\n",
    "                any([col.startswith(prefix) for prefix in self.prefixes])\n",
    "                & (col not in self.ignore)\n",
    "            )\n",
    "        ]\n",
    "        if self.zero_heavy:\n",
    "            X_ = X.copy().replace(0, np.nan)\n",
    "        else:\n",
    "            X_ = X.copy()\n",
    "        self.scaler = self.scaler.fit(X_[self.prefix_cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        X_[self.prefix_cols] = self.scaler.transform(X_[self.prefix_cols])\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: train-evaluate\n",
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-overflow: scroll\n",
    "#| code-summary: \"Mostrar/esconder código\"\n",
    "#| code-fold: true\n",
    "\n",
    "class TrainEvaluate:\n",
    "    \"\"\"\n",
    "    This class can be used to train, validate and test sklearn Pipeline objects.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: Pipeline, param_grid: dict, target: str,\n",
    "                 njobs: int = 8, verbose: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        model: sklearn Pipeline with the model.\n",
    "        param_grid: Dictionary of parameters to search over.\n",
    "        target: Name of the column to predict.\n",
    "        save_model: Wheter to save the model or not.\n",
    "        save_name: Name of the file to save the model.\n",
    "        njobs: Number of jobs to run in parallel.\n",
    "        verbose: Wheter to print the progress or not.\n",
    "        Initialize the class with the model, param_grid, and target variable.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.param_grid = param_grid\n",
    "        self.target = target\n",
    "        self.njobs = njobs\n",
    "        self.verbose = verbose\n",
    "        pass\n",
    "\n",
    "    def _validation_split(self, df: pd.DataFrame) -> tuple:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the data.\n",
    "        Split the data into train and validation sets.\n",
    "        \"\"\"\n",
    "        y = df[self.target]\n",
    "        X = df.drop(self.target, axis=1)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=0.25,\n",
    "            random_state=42\n",
    "        )\n",
    "        return (X_train, X_val, y_train, y_val)\n",
    "    \n",
    "    def _grid_search(self, X_train: pd.DataFrame, y_train: Union[pd.DataFrame, pd.Series]) -> GridSearchCV:\n",
    "        \"\"\"\n",
    "        X_train: Pandas DataFrame with the training data.\n",
    "        y_train: Pandas Series with the training target.\n",
    "        \"\"\"\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=self.model,\n",
    "            param_grid=self.param_grid,\n",
    "            scoring=\"roc_auc\",\n",
    "            n_jobs=self.njobs,\n",
    "            cv=skf\n",
    "        )\n",
    "        grid_search = grid_search.fit(X_train, y_train)\n",
    "        return grid_search\n",
    "    \n",
    "    def _profit(self, y_true: Union[np.ndarray, pd.DataFrame, pd.Series],\n",
    "                y_pred: Union[np.ndarray, pd.DataFrame, pd.Series]) -> float:\n",
    "        \"\"\"\n",
    "        y_true: Pandas Series with the true target.\n",
    "        y_pred: Pandas Series with the predicted target.\n",
    "        Calculate the profit metric of the model.\n",
    "        \"\"\"\n",
    "        tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        n = len(y_true)\n",
    "        profit = (90 * tp - 10 * fp)\n",
    "        return profit\n",
    "    \n",
    "    def _threshold_tuning(self, X_val: pd.DataFrame, y_val: Union[pd.DataFrame, pd.Series]) -> float:\n",
    "        \"\"\"\n",
    "        X_val: Pandas DataFrame with the validation data.\n",
    "        y_val: Pandas Series with the validation target.\n",
    "        Find the threshold that maximizes the profit metric.\n",
    "        \"\"\"\n",
    "        y_proba = self.best_model_.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        def profit_treshold(x: float) -> float:\n",
    "            \"\"\"\n",
    "            x: Threshold to test.\n",
    "            Returns negative of the profit metric.\n",
    "            \"\"\"\n",
    "            y_pred = (y_proba >= x).astype(int)\n",
    "            scalar = -self._profit(y_val, y_pred)\n",
    "            return scalar\n",
    "        \n",
    "        threshold = minimize_scalar(\n",
    "            profit_treshold,\n",
    "            bounds=(0, 1),\n",
    "            method=\"bounded\"\n",
    "        )\n",
    "        self.threshold = threshold.x\n",
    "        return threshold.x\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the data.\n",
    "        path: Path to a fitted model.\n",
    "        Splits data between train and validation, performs GridSearchCV,\n",
    "        adjusts the threshold based on profit metric on the validation set,\n",
    "        and fits the model on the original data.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Splitting data into train and validation sets...\")\n",
    "        X_train, X_val, y_train, y_val = self._validation_split(df)\n",
    "        if self.verbose:\n",
    "            print(\"Done!\")\n",
    "            print(\"Performing GridSearchCV...\")\n",
    "        self.best_model_ = self._grid_search(X_train, y_train).best_estimator_\n",
    "        if self.verbose:\n",
    "            print(\"Done!\")\n",
    "            print(\"Adjusting threshold based on validation set...\")\n",
    "        self.threshold = self._threshold_tuning(X_val, y_val)\n",
    "        if self.verbose:\n",
    "            print(\"Done!\")\n",
    "            print(\"Fitting model on the whole dataset...\")\n",
    "        self.best_model_ = self.best_model_.fit(df.drop(self.target, axis=1), df[self.target])\n",
    "        if self.verbose:\n",
    "            print(\"Done!\")\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the data.\n",
    "        Predicts the target variable using the best model.\n",
    "        \"\"\"\n",
    "        return self.best_model_.predict_proba(df)[:, 1]\n",
    "    \n",
    "    def predict(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the data.\n",
    "        Predicts the target variable using the best model and the threshold.\n",
    "        \"\"\"\n",
    "        y_proba = self.predict_proba(df)\n",
    "        y_pred = (y_proba >= self.threshold).astype(int)\n",
    "        return y_pred\n",
    "    \n",
    "    def evaluate(self, df: pd.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the test data.\n",
    "        Evaluates the model on the data.\n",
    "        \"\"\"\n",
    "        X_test = df.drop(self.target, axis=1)\n",
    "        y_true = df[self.target]\n",
    "        y_proba = self.predict_proba(X_test)\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "        self.business_metrics = {\n",
    "            \"Profit (Total)\": tp * 90 - fp * 10,\n",
    "            \"Profit (per Customer)\": (tp * 90 - fp * 10) / len(y_true),\n",
    "            \"True Positive Profit (Total)\": tp * 90,\n",
    "            \"True Positive Profit (per Customer)\": tp * 90 / len(y_true),\n",
    "            \"False Positive Loss (Total)\": fp * 10,\n",
    "            \"False Positive Loss (per Customer)\": fp * 10 / len(y_true),\n",
    "            \"False Negative Potential Profit Loss (Total)\": fn * 90,\n",
    "            \"False Negative Potential Profit Loss (per Customer)\": fn * 90 / len(y_true),\n",
    "            \"True Negative Loss Prevention (Total)\": tn * 10,\n",
    "            \"True Negative Loss Prevention (per Customer)\": tn * 10 / len(y_true)\n",
    "        }\n",
    "\n",
    "        self.classification_metrics = {\n",
    "            \"Classification Threshold\": self.threshold,\n",
    "            \"ROC AUC\": roc_auc_score(y_true, y_proba),\n",
    "            \"Precision\": precision_score(y_true, y_pred),\n",
    "            \"Recall\": recall_score(y_true, y_pred),\n",
    "            \"F1\": f1_score(y_true, y_pred),\n",
    "            \"Accuracy\": accuracy_score(y_true, y_pred)\n",
    "        }\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict_profit(self, model, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        X: Pandas DataFrame with the data.\n",
    "        y: Pandas Series with the target.\n",
    "        Predicts the profit metric using the best model and custom threshold.\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X)\n",
    "        return self._profit(y, y_pred)\n",
    "\n",
    "    def get_feature_importances(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the data.\n",
    "        Implements permutation feature importances on the data using the best model and custom threshold.\n",
    "        \"\"\"\n",
    "    \n",
    "        X = df.drop(self.target, axis=1)\n",
    "        X = self.best_model_.steps[0][1].transform(X)\n",
    "        y = df[self.target]\n",
    "        result = permutation_importance(\n",
    "            self.best_model_.steps[1][1],\n",
    "            X,\n",
    "            y,\n",
    "            scoring=self._predict_profit,\n",
    "            n_repeats=30,\n",
    "            random_state=42,\n",
    "            n_jobs=self.njobs\n",
    "        )\n",
    "        feature_importances = pd.DataFrame({\n",
    "            \"Feature\": X.columns,\n",
    "            \"Importance\": result.importances_mean\n",
    "        })\n",
    "        self.feature_importances = feature_importances.sort_values(\"Importance\", ascending=False)\n",
    "        return self.feature_importances\n",
    "    \n",
    "    def rank_customers(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with the data.\n",
    "        Ranks the customers by their probability of insatisfaction.\n",
    "        \"\"\"\n",
    "        df_ = df.copy()\n",
    "        X = df_.drop(self.target, axis=1)\n",
    "        y = df_[self.target]\n",
    "\n",
    "        def apply_rank(x: float) -> int:\n",
    "            \"\"\"\n",
    "            x: Probability of insatisfaction.\n",
    "            Applies the rank (1 to 5) to the probability of insatisfaction.\n",
    "            \"\"\"\n",
    "            thresholds = [c * self.threshold / 4 for c in range(5)][::-1]\n",
    "            for rank, threshold in enumerate(thresholds):\n",
    "                if x >= threshold:\n",
    "                    return rank + 1\n",
    "            return 5\n",
    "\n",
    "        df_[\"rank\"] = self.predict_proba(X)\n",
    "        return df_[\"rank\"].apply(apply_rank)\n",
    "    \n",
    "def build_model(path: str = None, train_df: pd.DataFrame = None, model: Pipeline = None,\n",
    "                param_grid: dict = None, target: str = None,\n",
    "                njobs: int = 8, verbose: bool = True) -> TrainEvaluate:\n",
    "    \"\"\"\n",
    "    path: Path to a fitted model.\n",
    "    train_df: Pandas DataFrame with the training data.\n",
    "    model: sklearn Pipeline with the model.\n",
    "    param_grid: Dictionary of parameters to search over.\n",
    "    target: Name of the column to predict.\n",
    "    njobs: Number of jobs to run in parallel.\n",
    "    verbose: Wheter to print the progress or not.\n",
    "    Builds a TrainEvaluate object.\n",
    "    \"\"\"\n",
    "\n",
    "    train_evaluate = TrainEvaluate(model, param_grid, target, njobs, verbose)\n",
    "    train_evaluate = train_evaluate.fit(train_df)\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(train_evaluate, f)\n",
    "    return train_evaluate"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
